{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87b850d02f11097c58dd2fcadb9473490f0fbec5"
   },
   "source": [
    "## SMS Spam Collection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f5829dce6459c732b1989fc648c2487c08136ed"
   },
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "884df48ea2c459c206c0afa5c375b07b7a320481"
   },
   "source": [
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06a0e1534b0a88594e3df8d4a5ffbea4b0379dad"
   },
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e73380ed292d21384e3409f8b7793f03bce7ffe"
   },
   "source": [
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "\n",
    "This corpus has been collected from free or free for research sources at the Internet:\n",
    "\n",
    "A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/. -> A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. -> A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a5a3441f25a0f7a9c14196369621a87e99327cc"
   },
   "source": [
    "### Contents of this notebook\n",
    "\n",
    "*  TEXT ANALYSIS\n",
    "        - Having a peek at the Data\n",
    "        - Developing Insights\n",
    "*  TEXT TRANSFORMATION\n",
    "        - Data Cleaning (Removing unimportant data/ Stopwords/ Stemming)\n",
    "        - Converting data into a model usable format (Bag of words/ Tfidf Model)\n",
    "*  MODEL APPLICATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c556276e9998629e9d3f9e68251ca4023b6bf552"
   },
   "source": [
    "#### TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54f4d31239a0647338b76c7d35f9eb115c8ab383"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Styles\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "# nltk.download(\"all\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ed1ededa351f22ba7d8e53c405cc67ebb5dfb8c"
   },
   "outputs": [],
   "source": [
    "messages = pd.read_csv(\"./../input/spam.csv\", encoding = 'latin-1')\n",
    "\n",
    "# Drop the extra columns and rename columns\n",
    "\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"category\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42bdc935918fe49044e615c0dfbac06e642da12a"
   },
   "outputs": [],
   "source": [
    "display(messages.head(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b85a7790c16cb92c0dc5af97389e8e745a76eef3"
   },
   "outputs": [],
   "source": [
    "# Lets look at the dataset info to see if everything is alright\n",
    "\n",
    "messages.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ce10b8024721b3c97a13f572a78145069f1ae6c",
    "collapsed": true
   },
   "source": [
    "##### Lets see what precentage of our data is spam/ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c38ff46dfb45305e6bcb4ef9613d35fb81e6dd9"
   },
   "outputs": [],
   "source": [
    "messages[\"category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n",
    "plt.ylabel(\"Spam vs Ham\")\n",
    "plt.legend([\"Ham\", \"Spam\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "463c4b88d40f44868714a92a6bc82e155f72e8a7"
   },
   "source": [
    "A lot of messages are actually not spam. About 86% of our dataset consists of normal messages.\n",
    "\n",
    "*  While we split our data set into train and test or when we use cross validation, we will have to use stratified sampling, otherwise we have a chance of our training model being skewed towards normal messages. If the sample we choose to train our model consists majorly of normal messages, it may end up predicting everything as ham and we might not be able to figure this out since most of the messages we get are actually ham and will have a pretty good accuracy.\n",
    "    \n",
    "* A very basic model would be a model that predicts everything as ham. It would have a decent accuracy. But then again, is that right? No. We will then have to use an accuracy metric that keeps this in mind. \n",
    "    Goal : We don't mind if we miss the odd spam message but we surely don't want to mark a ham message as spam i.e Precision is very important. Hence we will use fbeta score as our accuracy metric with inclination towards Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b171c976e5ec1b541da3238288bccc4ec1a85b5b",
    "collapsed": true
   },
   "source": [
    "##### Lets see the top spam/ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53f900816f9d4908099a6edd660ef38e11ec24f8"
   },
   "outputs": [],
   "source": [
    "topMessages = messages.groupby(\"text\")[\"category\"].agg([len, np.max]).sort_values(by = \"len\", ascending = False).head(n = 10)\n",
    "display(topMessages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dae7769ee74a422dafb6ad8ef9e449c4ca7fc921"
   },
   "source": [
    "So. People are really busy it seems. \"Sorry, i'll call later\" tops the ham message list with 30 counts with \"I cant pick the phone right now. Pls send a message\" comes second with 12 counts.\n",
    "\n",
    "Theres a quite lot of Ok..., Okie and Ok. in there too :>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e220cc356ffdb0df6d5d4c26b699640c24da44b",
    "collapsed": true
   },
   "source": [
    "##### Lets study individual Spam/ham words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7107711acdddde2d5d974141563d61ad017a9632"
   },
   "outputs": [],
   "source": [
    "spam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\n",
    "ham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\n",
    "\n",
    "spam_words = []\n",
    "ham_words = []\n",
    "\n",
    "# Since this is just classifying the message as spam or ham, we can use isalpha(). \n",
    "# This will also remove the not word in something like can't etc. \n",
    "# In a sentiment analysis setting, its better to use \n",
    "# sentence.translate(string.maketrans(\"\", \"\", ), chars_to_remove)\n",
    "\n",
    "def extractSpamWords(spamMessages):\n",
    "    global spam_words\n",
    "    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
    "    spam_words = spam_words + words\n",
    "    \n",
    "def extractHamWords(hamMessages):\n",
    "    global ham_words\n",
    "    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
    "    ham_words = ham_words + words\n",
    "\n",
    "spam_messages.apply(extractSpamWords)\n",
    "ham_messages.apply(extractHamWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ec079b4b6ca4486f8e033a3046821b5497115e0"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c6024d94d524500cac3398c35cd2e606a785887"
   },
   "outputs": [],
   "source": [
    "#Spam Word cloud\n",
    "\n",
    "spam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(spam_words))\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spam_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54fcc30698393716f19c016866312b5ad7d32ac"
   },
   "outputs": [],
   "source": [
    "#Ham word cloud\n",
    "\n",
    "ham_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(ham_words))\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(ham_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74025e5ea690679aef56752c7e8ecfcf9e9816f2"
   },
   "outputs": [],
   "source": [
    "# Top 10 spam words\n",
    "\n",
    "spam_words = np.array(spam_words)\n",
    "print(\"Top 10 Spam words are :\\n\")\n",
    "pd.Series(spam_words).value_counts().head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f17de5246259a3b3978aeafeb9d5d2af743b6264"
   },
   "outputs": [],
   "source": [
    "# Top 10 Ham words\n",
    "\n",
    "ham_words = np.array(ham_words)\n",
    "print(\"Top 10 Ham words are :\\n\")\n",
    "pd.Series(ham_words).value_counts().head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cacf007cf2d004b2c51273d31f5e370d3945e373",
    "collapsed": true
   },
   "source": [
    "#### Does the length of the message indicates us anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0b15b50fe828f9af19f5a5287f33bdab5af71f1"
   },
   "outputs": [],
   "source": [
    "messages[\"messageLength\"] = messages[\"text\"].apply(len)\n",
    "messages[\"messageLength\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89f11851732828c5fd43d91aa5feffabafe6f749"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize = (20, 6))\n",
    "\n",
    "sns.distplot(messages[messages[\"category\"] == \"spam\"][\"messageLength\"], bins = 20, ax = ax[0])\n",
    "ax[0].set_xlabel(\"Spam Message Word Length\")\n",
    "\n",
    "sns.distplot(messages[messages[\"category\"] == \"ham\"][\"messageLength\"], bins = 20, ax = ax[1])\n",
    "ax[0].set_xlabel(\"Ham Message Word Length\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ef76f1e0a1ee2cb5087f8535ca7f9d03a960a56"
   },
   "source": [
    "Looks like spam messages are usually longer. Maybe messageLength can become a feature to predict whether the message is spam/ ham ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4bb350aff1e1e5641c133c43477e5bc08826226",
    "collapsed": true
   },
   "source": [
    "#### TEXT TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b96c635ef437a18a2e20fc4aea0d8327e1c1bc3"
   },
   "source": [
    "#### Lets clean our data by removing punctuations/ stopwords and stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5075668a58e1a6ac3df8ed39fb6cb2608ff91c0b"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def cleanText(message):\n",
    "    \n",
    "    message = message.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "messages[\"text\"] = messages[\"text\"].apply(cleanText)\n",
    "messages.head(n = 10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acb2840effebd9ba6f58fba853568c519dd3c4d4"
   },
   "source": [
    "##### Lets convert our clean text into a representation that a machine learning model can understand. I'll use the Tfifd for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6df4f9e7df701b1eee1e2adf212fe457f09e9159"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\n",
    "features = vec.fit_transform(messages[\"text\"])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55093358b57afaa87db05552948518d4ef206c8f"
   },
   "source": [
    "#### MODEL APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "250ef549d64b975b0723a42ef3bad50e809daca9"
   },
   "outputs": [],
   "source": [
    "def encodeCategory(cat):\n",
    "    if cat == \"spam\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "messages[\"category\"] = messages[\"category\"].apply(encodeCategory)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, messages[\"category\"], stratify = messages[\"category\"], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b41983d5b4f3ac006f3a001d4f45b156fb2452d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "gaussianNb = MultinomialNB()\n",
    "gaussianNb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gaussianNb.predict(X_test)\n",
    "\n",
    "print(fbeta_score(y_test, y_pred, beta = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26e5b649f4ba8225b60b2578cff074e817ad11db",
    "collapsed": true
   },
   "source": [
    "TODO : Use more models/ Include Grid Search to find the most optimal model/ Use Lemma? / Use NER ? etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
